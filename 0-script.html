<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>script</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="0-script_files/libs/clipboard/clipboard.min.js"></script>
<script src="0-script_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="0-script_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="0-script_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="0-script_files/libs/quarto-html/popper.min.js"></script>
<script src="0-script_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="0-script_files/libs/quarto-html/anchor.min.js"></script>
<link href="0-script_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="0-script_files/libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="0-script_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="0-script_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="0-script_files/libs/bootstrap/bootstrap-d6a003b94517c951b2d65075d42fb01b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="script-double-debiased-machine-learning-for-causal-treatment-effects" class="level1">
<h1>Script: Double / Debiased Machine Learning for Causal Treatment Effects</h1>
<p>Audience: MSc Statistics students with graduate-level econometrics<br>
Duration: ~25 minutes</p>
<hr>
<section id="slide-1-title" class="level2">
<h2 class="anchored" data-anchor-id="slide-1-title">Slide 1 — Title</h2>
<p>“Hello everyone. Today I’ll talk about <em>Double / Debiased Machine Learning for Causal Treatment Effects</em>.</p>
<p>This is based on the paper by Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey and Robins.<br>
The goal is to show how to safely combine modern machine learning with classical econometric ideas to estimate causal effects.”</p>
<p>(Briefly introduce yourself and the seminar context.)</p>
<hr>
</section>
<section id="slide-2-the-problem-causal-effects-with-many-covariates" class="level2">
<h2 class="anchored" data-anchor-id="slide-2-the-problem-causal-effects-with-many-covariates">Slide 2 — The problem: causal effects with many covariates</h2>
<p>“Let me start with the basic problem.</p>
<p>We have observational data, a treatment or policy variable (D), an outcome (Y), and a big vector of potential confounders (X ^p).</p>
<p>We’re interested in a low-dimensional causal parameter (_0):<br>
for example an average treatment effect, or a regression coefficient with a causal interpretation, or an IV effect.</p>
<p>In low dimensions, we’d specify a parametric model and use OLS or IV.<br>
But when the dimension of (X) is large, those parametric models are easy to misspecify.</p>
<p>On the other hand, modern ML methods are extremely good at predicting (Y) from ((D,X)),<br>
but they are tuned to minimize prediction loss, not to give unbiased and ()-consistent estimators of (_0).</p>
<p>The central question of the talk is:<br>
**how can we combine ML with econometric identification so that we still get valid ()-inference for a causal parameter (_0)?**”</p>
<hr>
</section>
<section id="slide-3-prediction-vs.-causal-estimation" class="level2">
<h2 class="anchored" data-anchor-id="slide-3-prediction-vs.-causal-estimation">Slide 3 — Prediction vs.&nbsp;causal estimation</h2>
<p>“Let me make the distinction between prediction and causal estimation explicit.</p>
<p>Machine learning typically tries to approximate the regression function [ x [YX=x], ] or sometimes ([YD,X]). Algorithms like random forests, boosting and neural nets are optimized for prediction risk.</p>
<p>In causal inference we care about a <em>functional</em> (_0) of the distribution of ((Y,D,X)).<br>
A very common way to define (_0) is as the solution of a moment condition [ _P[(W;_0,_0)] = 0, W=(Y,D,X), ] for some score function () and some nuisance functions (_0).</p>
<p>A naive idea is: ‘Fit a very flexible ML model for (Y) on ((D,X)) and read off the effect of (D).’<br>
The problem is that the regularization bias that helps ML methods predict well can destroy the ()-behavior of the estimator of (_0).</p>
<p>I’ll make that precise using moment conditions and a simple bias decomposition.”</p>
<hr>
</section>
<section id="slide-4-why-econometricians-love-moment-conditions" class="level2">
<h2 class="anchored" data-anchor-id="slide-4-why-econometricians-love-moment-conditions">Slide 4 — Why econometricians love moment conditions</h2>
<p>“In econometrics, many parameters are defined through moment conditions.</p>
<p>For OLS we have [ [X(Y - X’_0)] = 0, ] for IV [ [Z(Y - D_0)] = 0, ] and GMM stacks many such conditions.</p>
<p>Formally, we think of parameters as solutions of [ [(W;_0,_0)] = 0. ]</p>
<p>That’s useful here for two reasons:</p>
<ol type="1">
<li>It separates the <strong>target</strong> (_0) from the <strong>nuisance</strong> (_0) (which we are happy to estimate with ML).</li>
<li>Concepts like orthogonal or efficient scores live naturally at the level of these moment conditions.</li>
</ol>
<p>So in this talk we start from moment conditions for (_0), and then ask:<br>
what happens when we plug in ML estimators for the nuisances?”</p>
<hr>
</section>
<section id="slide-5-moment-conditions-in-the-partially-linear-model" class="level2">
<h2 class="anchored" data-anchor-id="slide-5-moment-conditions-in-the-partially-linear-model">Slide 5 — Moment conditions in the partially linear model</h2>
<p>“Let me specialize to a workhorse model: the partially linear regression (PLR).</p>
<p>We assume [ Y = _0 D + g_0(X) + U,[UD,X]=0, ] [ D = m_0(X) + V,[VX]=0. ]</p>
<p>Here (_0) is the causal parameter: the effect of (D) on (Y) conditional on (X).<br>
The functions (g_0) and (m_0) are nuisance functions: high-dimensional and potentially nonlinear.</p>
<p>There are several natural moment conditions that identify (_0), assuming (g_0,m_0) are known.</p>
<ol type="1">
<li><p><strong>Regression adjustment:</strong> [ [(Y - D_0 - g_0(X))D]=0. ]</p></li>
<li><p><strong>Propensity-like adjustment:</strong> [ [(Y - D_0)(D - m_0(X))]=0. ]</p></li>
<li><p><strong>Neyman-orthogonal (residual) score:</strong> [ [(Y - D_0 - g_0(X))(D - m_0(X))]=0. ]</p></li>
</ol>
<p>If we had oracle access to (g_0) and (m_0), all three would lead to the same (_0).<br>
The interesting part is what happens when we use ML estimates instead.”</p>
<hr>
</section>
<section id="slide-6-key-idea-for-applied-work" class="level2">
<h2 class="anchored" data-anchor-id="slide-6-key-idea-for-applied-work">Slide 6 — Key idea (for applied work)</h2>
<p>“Before going deeper into the math, here is the practical high-level idea.</p>
<p>If you want to combine ML and econometrics for causal effects, do three things:</p>
<ol type="1">
<li>Express your parameter (_0) as the solution of a moment condition.</li>
<li>Choose a <strong>Neyman-orthogonal</strong> score: a score whose moment is locally insensitive to small errors in the nuisance functions.</li>
<li>Estimate the nuisances with your favourite ML methods, but use <strong>cross-fitting</strong> to avoid overfitting effects.</li>
</ol>
<p>This shifts ML errors to <strong>second order</strong> for the parameter of interest.<br>
You keep ()-rates and asymptotic normality, so standard inference goes through.</p>
<p>Operationally, you can think of DML as:<br>
use ML to residualize (Y) and (D), then regress residualized (Y) on residualized (D) in a way that respects the causal assumptions.</p>
<p>The structure of the talk matches this: - First, how orthogonal scores and bias decompositions work. - Second, evidence from simulations and an application to 401(k). - Finally, limitations and a checklist for using DML in practice.”</p>
<hr>
</section>
<section id="slide-7-naive-regression-adjustment-bias-decomposition" class="level2">
<h2 class="anchored" data-anchor-id="slide-7-naive-regression-adjustment-bias-decomposition">Slide 7 — Naive regression adjustment: bias decomposition</h2>
<p>“Let me start with the naive regression-adjustment moment: [ [(Y - D_0 - g_0(X))D]=0. ]</p>
<p>Suppose for a moment that we do sample splitting: we use one half of the sample to train an ML estimator (g) of (g_0), and the other half to estimate (_0).</p>
<p>On the second half, the plug-in estimator is [ _{} = ( D_i^2 )^{-1} ( D_i (Y_i - g(X_i)) ). ]</p>
<p>We can write [ (_{} - _0) = A_n + B_n, ] where [ A_n = (E[D^2])^{-1}D_i U_i ] is well behaved and asymptotically normal by the central limit theorem.</p>
<p>The problem is the second term [ B_n (E[D^2])^{-1}m_0(X_i)( g_0(X_i) - g(X_i)). ]</p>
<p>In high dimensions or with complex functions, ML estimators of (g_0) converge at rates (|g_0 - g|_{L_2} = n^{-_g}) with (_g &lt; 1/2).<br>
The term (B_n) is roughly of order (,n^{-_g}), which tends to infinity in general.</p>
<p>So even with sample splitting and quite good prediction, the regularization bias in (g) destroys ()-consistency of (_{}).”</p>
<hr>
</section>
<section id="slide-8-orthogonal-residual-score-bias-decomposition" class="level2">
<h2 class="anchored" data-anchor-id="slide-8-orthogonal-residual-score-bias-decomposition">Slide 8 — Orthogonal (residual) score: bias decomposition</h2>
<p>“Now let’s switch to the orthogonal score for the PLR model: [ (W;,g,m) = (Y - D - g(X))(D - m(X)). ]</p>
<p>The corresponding estimator () solves the empirical moment condition [ (W_i;,g,m) = 0, ] which gives the closed-form expression [ = {(D_i - m(X_i))^2}. ]</p>
<p>We again look at the scaled error: [ (- _0) = A_n^* + B_n^* + C_n^*. ]</p>
<p>The leading term is [ A_n^* = (E[V^2])^{-1} V_i U_i, ] which is asymptotically normal.</p>
<p>Crucially, the bias term now looks like [ B_n^* = (E[V^2])^{-1} (m(X_i) - m_0(X_i))(g(X_i) - g_0(X_i)). ]</p>
<p>So instead of having a <em>single</em> estimation error, we have a <em>product</em> of two estimation errors.<br>
If the ML rates are [ |m - m_0| = n^{-_m},|g - g_0| = n^{-_g}, ] then (B_n^*) is of order (,n^{-(_m+_g)}).</p>
<p>Whenever (_m + _g &gt; 1/2), this goes to zero.<br>
So the ML can be relatively slow, as long as the product of rates is good enough, and the bias is still second-order.”</p>
<hr>
</section>
<section id="slide-9-neyman-orthogonality-definition-plr-example" class="level2">
<h2 class="anchored" data-anchor-id="slide-9-neyman-orthogonality-definition-plr-example">Slide 9 — Neyman orthogonality: definition &amp; PLR example</h2>
<p>“Why does this miracle happen? The key property is Neyman orthogonality.</p>
<p>General set-up: we have a score ((W;,)), and ((_0,_0)) satisfies [ [(W;_0,_0)] = 0. ]</p>
<p>Consider the Gateaux derivative of the population moment with respect to the nuisance: [ <em>,[(W;_0,)]|</em>{=_0} [- _0]. ]</p>
<p>We say the score is <strong>Neyman-orthogonal</strong> if this derivative is zero for all admissible directions (-_0).</p>
<p>Intuitively, this means the moment condition is locally insensitive to small perturbations in the nuisance functions:<br>
plugging in slightly wrong () does not change the moment at first order.</p>
<p>In the PLR case, [ (W;,g,m) = (Y - D - g(X))(D - m(X)), ] and one can check that [ <em>{(g,m)},[(W;_0,g,m)]|</em>{(g,m)=(g_0,m_0)} = 0. ]</p>
<p>That orthogonality is exactly what lets us ‘kill’ the first-order bias and forces estimation error into the second order.”</p>
<hr>
</section>
<section id="slide-10-residual-interpretation-iv-style-view" class="level2">
<h2 class="anchored" data-anchor-id="slide-10-residual-interpretation-iv-style-view">Slide 10 — Residual interpretation &amp; IV-style view</h2>
<p>“An equivalent and very intuitive way to see the orthogonal score is via residuals.</p>
<p>Define [ Y_i := Y_i - g(X_i), D_i := D_i - m(X_i). ]</p>
<p>The empirical orthogonal moment condition is [ D_i(Y_i - D_i) = 0. ]</p>
<p>This says: in the regression of (Y_i) on (D_i), the residual is uncorrelated with (D_i).<br>
So [ Y_i = D_i + , [D_i ] = 0. ]</p>
<p>The solution is [ = . ]</p>
<p>This has an <strong>IV-style interpretation</strong>:<br>
(D_i = D_i - m(X_i)) behaves like an instrument for (D_i) after controlling flexibly for (X),<br>
and we regress the residualized outcome (Y) on the residualized treatment.</p>
<p>So DML is literally a bridge: ML learns the residualization, and then we plug into a very classical one-parameter IV/GMM-type estimator.”</p>
<hr>
</section>
<section id="slide-11-sample-splitting-and-cross-fitting" class="level2">
<h2 class="anchored" data-anchor-id="slide-11-sample-splitting-and-cross-fitting">Slide 11 — Sample splitting and cross-fitting</h2>
<p>“The last conceptual ingredient is sample splitting and cross-fitting.</p>
<p>The bias decomposition I showed hides a third remainder term, call it (C_n^*), which involves products like [ V_i(g(X_i) - g_0(X_i)), ] and similar terms with (m).</p>
<p>If we estimate (g) and (m) on the same data we use to form the score, these terms are difficult to control:<br>
the ML estimators can overfit, and the dependence structure is messy.</p>
<p>The solution is:</p>
<ol type="1">
<li>Split the sample into (K) folds.</li>
<li>On fold (k), estimate (g^{(-k)}) and (m^{(-k)}) using only the other folds.</li>
<li>On fold (k), compute residuals and an estimate (^{(k)}).</li>
<li>Average over (k).</li>
</ol>
<p>Conditional on the training folds, the nuisance errors and the residuals behave almost like independent objects.<br>
We can then bound terms like (C_n^*) with simple variance calculations.</p>
<p>Cross-fitting also means that in the end we’re using the whole sample to estimate (_0),<br>
so we don’t lose efficiency by splitting the data once and for all.”</p>
<hr>
</section>
<section id="slide-12-algorithm-for-plr-dml-12" class="level2">
<h2 class="anchored" data-anchor-id="slide-12-algorithm-for-plr-dml-12">Slide 12 — Algorithm for PLR DML (1/2)</h2>
<p>“Let me summarize the algorithm concretely for the PLR model.</p>
<ol type="1">
<li><p>Choose your ML methods for (g_0(X)) and (m_0(X)).<br>
For example, lasso, random forests, boosting, neural nets, or some ensemble.</p></li>
<li><p>Fix the number of folds (K), say (K=5).</p></li>
<li><p>Randomly partition the data indices ( {1,,n}) into folds (I_1,,I_K).</p></li>
<li><p>For each fold (k):</p>
<ol type="a">
<li>Train (g<sup>{(-k)},m</sup>{(-k)}) on all observations not in (I_k).<br>
</li>
<li>For each (iI_k) compute the residuals [ Y_i := Y_i - g^{(-k)}(X_i), D_i := D_i - m^{(-k)}(X_i). ]</li>
</ol></li>
</ol>
<p>On the next slide, we use these residuals to get (_{}).”</p>
<hr>
</section>
<section id="slide-13-algorithm-for-plr-dml-22" class="level2">
<h2 class="anchored" data-anchor-id="slide-13-algorithm-for-plr-dml-22">Slide 13 — Algorithm for PLR DML (2/2)</h2>
<p>“Continuing the algorithm:</p>
<ol start="5" type="1">
<li><p>For each fold (k), on the held-out data (iI_k), run a simple OLS regression of (Y_i) on (D_i) without an intercept: [ ^{(k)} := . ]</p></li>
<li><p>Aggregate the fold-specific estimates: [ <em>{} := </em>{k=1}^K ^{(k)}. ]</p></li>
<li><p>Finally, estimate the asymptotic variance using the empirical influence function based on the orthogonal score, and construct confidence intervals in the usual way.</p></li>
</ol>
<p>[If you add a flowchart here, you can visually show: raw data () ML fits of nuisances () residuals () regression of residuals.]”</p>
<hr>
</section>
<section id="slide-14-what-double-ml-delivers-high-level" class="level2">
<h2 class="anchored" data-anchor-id="slide-14-what-double-ml-delivers-high-level">Slide 14 — What double ML delivers (high level)</h2>
<p>“Let me now summarize what double ML buys us, theoretically.</p>
<p>Under mild rate conditions on the ML estimators — essentially that the nuisance estimators are consistent and the product of their rates is fast enough — we have:</p>
<ul>
<li><p>(<em>{}) is ()-consistent and asymptotically normal: [ (</em>{} - _0) (0,^2). ]</p></li>
<li><p>We can estimate (^2) via the sample variance of the orthogonal score, and form standard Wald confidence intervals.</p></li>
</ul>
<p>The nice feature is that this holds for a wide range of ML methods:<br>
lasso, random forests, boosting, neural nets, GAMs, and ensembles.</p>
<p>In the PLR model under homoscedasticity, the DML estimator actually attains the semiparametric efficiency bound.<br>
So in that sense, we lose nothing by using ML for nuisance estimation, provided we use orthogonal scores and cross-fitting.”</p>
<hr>
</section>
<section id="slide-15-simulation-prediction-vs.-causal-estimation" class="level2">
<h2 class="anchored" data-anchor-id="slide-15-simulation-prediction-vs.-causal-estimation">Slide 15 — Simulation: prediction vs.&nbsp;causal estimation</h2>
<p>“Chernozhukov and co-authors illustrate the difference between naive plug-in and DML using simulations.</p>
<p>In one design, they construct (g_0(X)) as a function that is particularly friendly to random forests — effectively a combination of tree structures — so that random forests are almost oracle for prediction.</p>
<p>They then compare two estimators of (_0):</p>
<ol type="1">
<li>Naive ML plug-in: use a random forest prediction model for (Y) on ((D,X)), and read off the coefficient or partial dependence for (D).</li>
<li>The DML estimator based on the orthogonal score and cross-fitting.</li>
</ol>
<p>The results are:</p>
<ul>
<li><p>The naive plug-in estimator predicts (Y) extremely well, but its sampling distribution for (_0) is badly biased: the histogram of (- _0) is shifted away from zero.</p></li>
<li><p>The DML estimator’s histogram is centered at zero and well approximated by a normal curve.</p></li>
</ul>
<p>The lesson is: <strong>excellent prediction error does not imply good causal estimation</strong>.<br>
The structure of the score — in particular, orthogonality — is what protects the causal parameter.”</p>
<hr>
</section>
<section id="slide-16-application-401k-eligibility-and-savings" class="level2">
<h2 class="anchored" data-anchor-id="slide-16-application-401k-eligibility-and-savings">Slide 16 — Application: 401(k) eligibility and savings</h2>
<p>“Let me briefly discuss an empirical illustration: 401(k) eligibility and household savings.</p>
<p>We consider US survey data where:</p>
<ul>
<li>(Y): net financial assets — including 401(k), IRAs, checking, stocks, minus non-mortgage debt.</li>
<li>(D): indicator for being <em>eligible</em> for a 401(k) plan at the current job.</li>
<li>(X): a rich set of covariates such as income, age, education, family size, marital status, defined benefit pension status, IRA participation, and home ownership.</li>
</ul>
<p>Identification strategy (following the literature): conditional on these covariates, eligibility can be treated as as-good-as-random.<br>
So we aim at the average treatment effect of eligibility on net assets.</p>
<p>We use DML for this ATE:</p>
<ul>
<li>ML methods estimate outcome regressions and possibly propensity scores as functions of (X).</li>
<li>We plug them into an orthogonal score and apply cross-fitting.</li>
</ul>
<p>Empirically, across a range of learners (lasso, trees, forests, boosting, nets, ensembles),<br>
the estimated ATE is substantial and positive, on the order of ($7)–(9)k in extra net financial assets for being eligible.</p>
<p>The key point here is not the exact number, but that we are able to flexibly control for a high-dimensional (X) with ML<br>
<strong>and</strong> still get standard errors and confidence intervals for the causal effect.”</p>
<hr>
</section>
<section id="slide-17-beyond-plr-ate-atte-pliv-late" class="level2">
<h2 class="anchored" data-anchor-id="slide-17-beyond-plr-ate-atte-pliv-late">Slide 17 — Beyond PLR: ATE, ATTE, PLIV, LATE</h2>
<p>“The PLR model is just the starting point.</p>
<p>The same DML blueprint extends to several other core causal estimands.</p>
<ol type="1">
<li><p><strong>ATE / ATTE under unconfoundedness.</strong><br>
We observe (Y,D,X) with binary (D), and assume that conditional on (X), treatment is as-good-as-random.<br>
Orthogonal scores combine outcome regression (g_0(d,X)) and the propensity score (m_0(X) = [DX]).<br>
Again, we estimate nuisances with ML and plug them into the orthogonal score with cross-fitting.</p></li>
<li><p><strong>Partially linear IV (PLIV).</strong><br>
Here we have an instrument (Z) and model [ Y = _0 D + g_0(X) + U,[UX,Z]=0, ] [ D = m_0(X,Z) + V,[VX,Z]=0. ] The exclusion restriction is that, given (X), (Z) affects (Y) only through (D).<br>
DML uses an orthogonal score that involves the residualized outcome and a residualized version of the instrument.</p></li>
<li><p><strong>LATE with binary (D) and (Z).</strong><br>
In the local average treatment effect setting, the parameter is a ratio of two expectations (a Wald estimand).<br>
DML constructs orthogonal scores for numerator and denominator, with ML for nuisance functions such as (_0(z,x) = [YZ=z,X=x]), (m_0(z,x) = [DZ=z,X=x]), and the instrument propensity.</p></li>
</ol>
<p>In all these settings, the pattern is the same:<br>
low-dimensional causal parameter, high-dimensional nuisance functions, orthogonal score, ML, cross-fitting.”</p>
<hr>
</section>
<section id="slide-18-what-dml-does-not-fix" class="level2">
<h2 class="anchored" data-anchor-id="slide-18-what-dml-does-not-fix">Slide 18 — What DML does <em>not</em> fix</h2>
<p>“So far I’ve emphasized what DML does well. Let me be explicit about what it does <em>not</em> solve.</p>
<p>DML relaxes <em>functional-form</em> assumptions by allowing ML to estimate complex nuisance relationships.<br>
But it does not relax the underlying <strong>causal identification assumptions</strong>:</p>
<ul>
<li>In PLR or ATE settings, we still need that all confounders are observed and appropriately controlled for in (X).</li>
<li>In IV or LATE settings, we still need valid instruments (Z): relevance, exclusion, and monotonicity or other structural conditions where required.</li>
<li>We still need sufficient overlap or positivity: we shouldn’t have many units with propensity scores extremely close to 0 or 1.</li>
<li>We must avoid ‘bad controls’ — variables that are colliders or mediators in the causal graph.</li>
</ul>
<p>If important confounders are unobserved, DML cannot magically remove the resulting bias.</p>
<p>So ML plus DML is not a substitute for good research design. You still have to think carefully about the causal structure and variable choice.”</p>
<hr>
</section>
<section id="slide-19-evidence-from-method-evaluation-studies" class="level2">
<h2 class="anchored" data-anchor-id="slide-19-evidence-from-method-evaluation-studies">Slide 19 — Evidence from method evaluation studies</h2>
<p>“There are also some lessons from method evaluation studies that are useful for practice.</p>
<p>Simulation papers that compare OLS, naive ML, and DML across different designs typically find:</p>
<ul>
<li><p>When confounding is truly linear and the parametric model is correctly specified, OLS and DML both perform well.<br>
In such cases, DML doesn’t buy you much beyond robustness to mild misspecification.</p></li>
<li><p>When confounding is nonlinear — say interactions, U-shaped effects, or thresholds —<br>
DML shines <strong>if</strong> you use flexible learners for the nuisance functions.</p>
<p>If you run DML with a very rigid learner, such as plain lasso on raw covariates and no transformations,<br>
you can get OLS-like bias because the nuisance fits are poor.</p>
<p>If you instead use flexible methods like random forests, boosting, GAMs, or neural nets,<br>
DML can substantially reduce bias while keeping variance under control.</p></li>
</ul>
<p>The main message: the ‘double’ in double ML does not guarantee good performance by itself.<br>
You still need to choose ML methods that are appropriate for the structure of your problem.”</p>
<hr>
</section>
<section id="slide-20-checklist-take-home-message" class="level2">
<h2 class="anchored" data-anchor-id="slide-20-checklist-take-home-message">Slide 20 — Checklist &amp; take-home message</h2>
<p>“Let me finish with a checklist and a summary you can take into your own projects.</p>
<p><strong>When is DML a good fit?</strong></p>
<ul>
<li>You have many covariates and believe the nuisance structure is complex but learnable.</li>
<li>You care about a low-dimensional parameter: ATE, ATTE, a regression coefficient in a PLR model, an IV effect, a LATE.</li>
<li>You have enough data to fit flexible ML models reasonably well.</li>
</ul>
<p><strong>What should you actually do?</strong></p>
<ol type="1">
<li>Write down the causal estimand and the moment condition that identifies it.</li>
<li>Derive or look up an orthogonal score for that estimand.</li>
<li>Choose ML algorithms capable of capturing the nonlinearities you care about.</li>
<li>Use cross-fitting and examine sensitivity to ML choices and to the number of folds (K).</li>
<li>Always think about the identification assumptions in parallel — ML cannot fix a bad instrument or missing confounder.</li>
</ol>
<p>If you remember just one sentence, let it be this:</p>
<blockquote class="blockquote">
<p>Use ML where it’s strong — in estimating high-dimensional nuisance functions —<br>
but protect the causal parameter with orthogonal moments and cross-fitting,<br>
so that ML errors only show up as second-order bias.</p>
</blockquote>
<p>Thanks, and I’m happy to discuss how this applies to your own empirical settings.”</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>